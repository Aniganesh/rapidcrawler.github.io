<!DOCTYPE html>
<html>

<head>
    <script type='text/javascript'
        src='https://platform-api.sharethis.com/js/sharethis.js#property=61684b19e876080012645f8a&product=sop'
        async='async'></script>
    <!-- NavBar . Also copy the part from start of <body> tag below -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <!-- page specific edits -->
    <script>
        $(function () {
            $("#nav_bar_dark").load("/assets/scripts/nav_bar_dark.html");
            $("#nlp_navbar").load("/assets/scripts/LHSSidebar/NLP_LHSNavBar.html");
        });
    </script>
    <!-- page specific edits -->
    <title>NLP: Attention</title>

    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/assets/stylesheets/nav_bar_dark_style.css">
    <link rel="stylesheet" type="text/css" href="/assets/stylesheets/blog_posts_style.css">
    <link rel="stylesheet" type="text/css" href="/assets/stylesheets/blog_posts_style.css">
    <link rel="stylesheet" type="text/css" href="/assets/stylesheets/button_style.css">
    <link rel="stylesheet" type="text/css" href="/assets/stylesheets/css_dark_mode_gist_embed_code.css">

    <!-- Google Fonts: Robot Black 900-->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@900&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Exo+2:wght@900&display=swap" rel="stylesheet">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

</head>

<body>
    <!-- NavBar . Also copy the part from start of <head> tag above -->
    <div id="nav_bar_dark"></div>

    <div class="three_col_layout">

        <!-- ####################
        LHSNavBar . Also copy the part from start of <head> tag above
        #################### -->
        <div class="column_left">
            <h2 style="text-align: left;"> &nbsp;&nbsp; Chapters </h2>
            <!-- page specific edits -->
            <div id="nlp_navbar"></div>
        </div>

        <!-- ####################
        Page Titke needs update
        #################### -->
        <div class="column_center">
            <h1>Natural Language Processing</h1>
            <div class="meta_info">
                <span style="float: left;">
                    <p>
                        <!-- ####################
                        Date-Posted needs update
                        #################### -->
                        Posted by: <a href="/index" target="_blank" class="blog_links">Jaspreet</a>
                    </p>
                </span>
                <span style="float: right;">
                    <p>
                        <!-- page specific edits -->
                        Last Updated on: 19 May, 2023
                    </p>
                </span>
            </div>


            <!-- ####################
            Banner img
            #################### -->
            <div class="banner">
                <!-- page specific edits -->
                <img src="/assets/imgs/banners/generic_banner.jpg">
            </div>

            <br>

            <div class="share_this_plugin">
                <br>
                <!-- Also copy the part from start of <head> tag above -->
                <div class="sharethis-inline-share-buttons"></div>
            </div>
            <!-- page specific edits -->
            <h1>Attention Based NLP Models</h1>
            <hr>
            <div class="hide_in_pc">
                <h3>
                    Chapters
                </h3>
                <!-- page specific edits -->
                <div id="nlp_navbar"></div>
                <hr>
            </div>
            <!-- ####################
              page specific edits
              #################### -->
            <div class="toc">
                <h1>Table of Contents</h1>
                <ol>
                    <li><a href="#fundamentals">Fundamentals</a></li>
                    <li><a href="#classics">Classics</a></li>
                </ol>


                <!-- <a href="#footer">3) Fun</a> <br> -->
            </div>
            <hr>

            <div class="blog_content">
                <br>
                <div id="fundamentals">
                    <h1>Fundamentals</h1>
                    <p>
                    <h2>Attention</h2>
                    The Attention mechanism is a fundamental component of many machine learning models, particularly in
                    natural language processing (NLP) tasks such as:
                    <ul>
                        <li>machine translation</li>
                        <li>text summarization</li>
                        <li>question answering</li>
                    </ul>
                    It allows the model to focus on specific parts of the input sequence while
                    making predictions.

                    <br><br>

                    Imagine you have a sentence in a foreign language that you want to translate into your native
                    language. Instead of translating the entire sentence in one step, you might <b>focus on one word at
                        a
                        time and consider its relation to the words in the source sentence</b>. This is what attention
                    does - it
                    selectively attends to different parts of the input sequence to gather relevant information for
                    making predictions.

                    <br><br>

                    The attention mechanism typically involves three main components:
                    <ul>
                        <li>a query</li>
                        <li>a set of key-value pairs</li>
                        <li>a scoring function</li>
                    </ul>
                    The query represents the current position or context, the key-value pairs
                    capture the input sequence, and the scoring function computes the relevance or importance of each
                    key-value pair with respect to the query.

                    <br><br>

                    Here's a step-by-step breakdown of how attention works:
                    <ol>
                        <li>
                            <b>Query Generation</b>: <br>
                            The input sequence is processed by an encoder, which produces a set of queries. Each query
                            represents a specific position or context in the input.
                        </li>
                        <li>
                            <b>Key-Value Pairs</b>: <br>
                            The encoder also generates a set of key-value pairs from the input sequence. The keys and
                            values can
                            be the same or different, depending on the specific attention mechanism used.
                        </li>
                        <li>
                            <b>Scoring</b>: <br>
                            The scoring function calculates a similarity score between each query and all the keys. The
                            score
                            reflects the relevance or importance of each key-value pair for the given query. Common
                            scoring
                            functions include dot product, additive/multiplicative attention, and cosine similarity.
                        </li>
                        <li>
                            <b>Attention Weights</b>: <br>
                            The scores are transformed into attention weights using a softmax function, which ensures
                            that the
                            weights sum up to 1 and represent the importance distribution over the key-value pairs for a
                            specific query.
                        </li>
                        <li>
                            <b>Weighted Sum</b>: <br>
                            The attention weights are applied to the corresponding values, producing a weighted sum.
                            This step
                            captures the relevant information from the input sequence based on the attention weights.
                        </li>
                        <li>
                            <b>Context Vector</b>: <br>
                            The weighted sum, also known as the context vector, represents the attended information from
                            the
                            input sequence. It contains the relevant features or representations that are most important
                            for
                            making predictions at the current position.
                            <br>
                            The context vector is then used in subsequent steps of the model to generate the desired
                            output, such as predicting the next word in machine translation or generating a summary.
                        </li>
                    </ol>
                    <h2>Self-Attention (Transformer)</h2>
                    Self-attention, also known as intra-attention or scaled dot-product attention, is a specific type of
                    attention mechanism used in the Transformer model, which has revolutionized NLP tasks.
                    Self-attention allows the model to attend to different positions within the same input sequence.
                    <br>
                    In a traditional attention mechanism, the queries, keys, and values come from different sources, but
                    in self-attention, they all come from the same input sequence. The goal is to understand the
                    dependencies and relationships between different positions or words within the sequence.
                    <br>
                    Here's how self-attention works in the Transformer model:
                    <ol>
                        <li>
                            <b>Input Embeddings</b>:
                            The input sequence is transformed into a set of embeddings, which capture the semantic
                            meaning of
                            each word or token. These embeddings are often a combination of word embeddings and
                            positional
                            encodings to convey both the word information and its position in the sequence.
                        </li>
                        <li>
                            <b>Query, Key, and Value Generation</b>:
                            The embeddings are linearly transformed into query, key, and value vectors. This
                            transformation
                            provides a learned representation for each position in the sequence, allowing the model to
                            compute
                            self-attention scores.
                        </li>
                        <li>
                            <b>Scoring</b>:
                            For each position in the input sequence, the self-attention mechanism calculates a
                            similarity score
                            between the query at that position and all other keys in
                        </li>
                        <li>
                            <b>Attention Weights</b>:
                            The scores are transformed into attention weights using a softmax function, generating a
                            distribution over all the positions in the input sequence. The weights indicate the
                            importance
                            of each position for the given position.
                        </li>
                        <li>
                            <b>Weighted Sum</b>:
                            The attention weights are applied to the corresponding values, producing a weighted sum.
                            This
                            step captures the relevant information from different positions within the input sequence
                            based
                            on their importance weights.
                        </li>
                        <li>
                            <b>Context Vector</b>:
                            The weighted sum, or the context vector, represents the attended information from the input
                            sequence for a specific position. It contains the relevant features or representations that
                            are
                            most important for making predictions at that position.
                        </li>
                    </ol>
                    The self-attention mechanism is applied multiple times in parallel across all positions in the
                    input sequence, allowing each position to attend to other positions and aggregate the relevant
                    information. The resulting context vectors are then used in subsequent layers of the Transformer
                    model to generate the desired output.

                    <br><br>

                    In summary, attention mechanisms, including self-attention, enable machine learning models to
                    selectively focus on different parts of the input sequence and gather the most relevant
                    information for making predictions. These mechanisms have significantly improved the performance
                    of various NLP tasks and have become integral components of state-of-the-art models like the
                    Transformer.
                    </p>
                </div>

                <div id="classics">
                    <h1>Classics</h1>
                    <p>
                    <ol>
                    </ol>
                    </p>
                </div>
            </div>
        </div>

        <div class="column_right">
        </div>
        <!-- ########################################### -->
        <!-- ############## RHS Sidebar ################ -->
        <!-- ########################################### -->

    </div>
    <!-- ################################################### -->
    <!-- ############ three_col_layout Ends Here ########### -->
    <!-- ################################################### -->

    <div style="height: 50px;">
        <h1></h1>
    </div>

    <div class="footer" id="footer">
        <div id="page_change_btns">
            <a href="/blog/learning_resources/ml_dl/nlp_resources">
                <!-- page specific edits -->
                <button class="pagination_button previous_button" style="vertical-align:middle;">
                    <span>Previous </span>
                </button>
            </a>

            <!-- ####################
                Link in nxt-btn needs update
                #################### -->
            <!-- page specific edits -->
            <a href="/blog/learning_resources/ml_dl/time_series.html">
                <button class="pagination_button next_button" style="">
                    <span>Next </span>
                </button>
            </a>
        </div>
        <br> <br> <br> <br>
        <div class="share_this_plugin">
            <h1>Share This</h1>
            <p>Share it with your friends and colleagues whom you think it can help</p>
            <!-- Footer . Also copy the part from start of <head> tag above -->
            <div class="sharethis-inline-share-buttons"></div>
        </div>

        <div class="comment_box">
            <!-- ########################################### -->
            <!-- ############# Needs updates ############### -->
            <!-- ########################################### -->
            <!-- begin wwww.htmlcommentbox.com -->
            <div id="HCB_comment_box"><a href="http://www.htmlcommentbox.com">Comment Box</a> is loading comments...
            </div>
            <link rel="stylesheet" type="text/css"
                href="https://www.htmlcommentbox.com/static/skins/bootstrap/twitter-bootstrap.css?v=0" />
            <script type="text/javascript"
                id="hcb"> /*<!--*/ if (!window.hcb_user) { hcb_user = {}; } hcb_user.PAGE = "https://juspreet51.github.io/blog/deep_learning/network_understanding_basic"; (function () { var s = document.createElement("script"), l = hcb_user.PAGE || ("" + window.location).replace(/'/g, "%27"), h = "https://www.htmlcommentbox.com"; s.setAttribute("type", "text/javascript"); s.setAttribute("src", h + "/jread?page=" + encodeURIComponent(l).replace("+", "%2B") + "&mod=%241%24wq1rdBcg%24WGLJ0qbFjhsr.BXys0KRJ%2F" + "&opts=16406&num=5&ts=1633641065176"); if (typeof s != "undefined") document.getElementsByTagName("head")[0].appendChild(s); })(); /*-->           */</script>
            <!-- end www.htmlcommentbox.com -->
        </div>
    </div>

</body>

</html>